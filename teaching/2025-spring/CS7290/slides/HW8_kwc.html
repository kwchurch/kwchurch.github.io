<html>
  <body>
<h1>HW8 Feedback</h1>

<p><b><font color="red"> As before, my comments will be in red.  If you don't see your abstract
here, feel free to contact the TA (Jianpu Ma jm4437@columbia.edu) to
make sure that he graded it.
</font></b></p>

<ol>
  <li>
<h2>Need to Understand What Machine Learning is Doing</h2>

	With the advent of deep learning, a greater emphasis should be
	placed on understanding the underlying algorithms and
	mathematical formulas behind machine learning models. In order
	to do so, the machine learning curriculum at universities
	needs to focus less on the theoretical underpinnings of older
	machine learning techniques and place greater emphasis on
	modern machine learning tools, like artificial neural
	networks, and on the real-world consequences of building
	poorly constructed models. In the process, coursework should
	become more interdisciplinary and experiential, with
	application-based projects, as well as sections and classes
	related to sociology, ethics, and philosophy. This will allow
	students to better understand the people who are being
	"modeled" by machine learning and the ramifications of
	building intellectually dishonest machine learning models.
	<p><b><font color="red">
I agree.  It would be nice to see more understanding.  Can you offer some constructive suggestions for how to get there?
</font></b></p>
  </li>

  <li>
<h2>More Data May Not Be Better</h2> 

More data is always better data, except for when it is
not. In her book "Weapons of Math Destruction," Cathy O'Neil brings to
light multiple examples of applications of machine learning algorithms
gone awry, and warns us of the dangerous assumption that models
trained on large datasets produce true results. She points out that
models are built by humans who, unavoidably, introduce their own
biases and prejudices in every step of the process, from data
collection to selection of features. Although models are just
algorithms with no intrinsic sense of good or evil, they become
dangerous when they are used to classify humans as "good" or "bad" on
a massive scale. The models can exacerbate the problem, feeding into
the biases of its creators and creating more data that mirrors the
biases of the data they were trained on, thus creating a vicious
feedback loop. The models are often unavailable to scrutiny from those
they misclassified, and can be difficult to understand even by their
own creators. Fortunately, not all hope is lost. In this survey, we
will examine challenges in identifying biases and approaches to
increase transparency of these processes. More data may be better
data, but better understanding of the models is even better.

	<p><b><font color="red"> 

The topic sentence starts off with a nice hook, and promises to offer
constructive suggestions.  I wish we had more than promises, but I'll
take what I can get.

</font></b></p>
  </li>
  <li>
<h2>Are Deep Nets Exposed to Adversarial Attacks?</h2>

We argue that the lack of explanatory power in deep learning systems
makes them open to adversarial attacks.  Many decisions in social
systems today--such as teacher evaluation, distributing police
patrolling, and hiring--rely on black-box algorithms that lack
explanatory power.  The outputs of these algorithms often result in
widespread and harmful biases.  In this paper, we review the research
on adversarial attacks on deep learning systems from the perspective
of how they could play out in social system settings.  We draw a
connection between the lack of explanatory power of an algorithm and
its ability to be misused and abused in a social system setting.

	<p><b><font color="red">
Nice pivot from WMD to what we've discussed recently.
</font></b></p>
  </li>

  <li>
<h2>Algorithmic Discrimination and the Profit Motive</h2> 

O'Neil (2017) illustrates the myriad ways in which "weapons
of math destruction" -- widespread and harmfully-opaque algorithms --
determine important outcomes in our lives, from employment to loan
eligibility. The book's conclusion posits that "We have to explicitly
embed better values into our algorithms, creating Big Data models
that follow our ethical lead. Sometimes that will mean putting
fairness ahead of profit." In this paper, we argue instead that the
profit motive is fundamentally incompatible with algorithmic fairness,
making the ideal of “putting fairness ahead of profit” quixotic at
best and deluded at worst under present conditions. In particular, we
examine the legal scholarship on corpo- rate “shareholder primacy”1,
pointing out the consistency with which courts have levied large
financial penalties against corporations for violating their
profit-maximization mandate. We then examine the 2008 financial
crisis, the present-day opioid crisis, and other crises involving
corporate malfeasance as examples of the futility of attempts to "put
fairness ahead of profit" via government regulation in the era of
neoliberalism. We conclude by arguing that the only way to confront
the growth of "weapons of math destruction" is to place decisions
regarding user-generated data under direct democratic control.
<p><b><font color="red">
      It is nice to see both a problem description as well as a constructive suggestion for solving that problem.
      I'd like to see more discussion on how democracy can help address this problem.

</font></b></p>
  </li>


  <li>
<h2>More Transparency is Needed</h2>

Although we have studied algorithms and technologies that have pushed
different areas forward like speech processing and analysis, many
algorithms applied in today's world often go understated despite their
effect on our everyday lives. Seeing our speech turn into words on our
screen is instantaneous, so we somewhat understand the technology in
front of us, but in a world where so many decisions are made for us
based on how we answer certain forms and profiles, it is impossible
for us to even notice the underlying technology that "ranks" us based
on our answers. In "Weapons of Math Destruction", O'Neil goes far
deeper into this by looking at different industries and seeing how
machine learning places people into different groups, questioning the
inherent "fairness" in these processes since we do not know their
inner workings and they have quite a bit of role in our lives. I agree
with O'Neil in suggesting that we have more transparency in these
algorithms and with companies in general, since as machine learning
gets better only more important decisions are going to be made
automatically.

	<p><b><font color="red">

</font></b></p>
  </li>

  <li>
<h2>Rich Get Richer</h2>

The big data models discussed in "Weapons of Math Destruction" in some
ways, seem to very well model the nature of the Matthew Effect that is
happening in the tech field.  Big tech companies and well known fast
growing startups that perform well end up attracting more and more
talents, which make them grow bigger and faster, while tech companies
that are perceived as "outdated" and "failing" perform poorly and
their valuations shrink continuously.  This idea of rich get richer
and the poor gets poorer also applies to individuals and also academia
in general. People who are able to attract attention due to their
software engineering skills and academic publications end up garnering
more resources.  Papers that start to get citations end up getting
more and more citations.  This perpetuating cycles seem to be
dominating the current tech world.

	<p><b><font color="red">
	      Nice pivot from WMD to citations.  I didn't know "Rich get richer" was known as the Matthew Effect.
	      The topic sentence could do a better job of introducing the pivot in the conclusion.
</font></b></p>
  </li>

    <li>
<h2>Machine Learning With Regulation</h2>

The survey discussed the side effects machine learning caused in the
real world. The survey concluded that our society may need some
policies to keep the fairness and to regulate those machine learning
applications. The survey mentioned some examples in the book Weapons
of Math Destruction. Those models are opaque, in large size and
sometimes destructive. If the inventor who decided the models, the
cost function didn't strictly obey the fairness conception, the model
would become a destruction. The survey pointed out a series [serious? -- kwc] social
problem like loan, risk evaluation unfairness and their
consequences. Finally, the survey listed some suggestions about how to
build a system to regulate those mathematical models' applications.

	<p><b><font color="red">

I like specifics like the mention of loans.  It would be good to see
more such specifics.  The connection to those specifics could be
clearer in the topic and concluding sentences.

</font></b></p>
  </li>

      <li>
<h2>Test Scores, GPAs, and Other College Admissions Data: Too Much of a Bad Thing</h2>

"There's no data like more data," if the goal is convoluting the
undergraduate college admissions process. As the number of applicants
grows and rankings remain prevalent, colleges will need to find a way
to efficiently evaluate the qualities they value most in their
applicants. Proxies -- such as SAT scores a proxy for aptitude -- are
one solution, but are subject to the pitfalls of feedback loops
existent in what Cathy O'Neil describes as "Weapons of Math
Destruction." The feedback loop inherent in college rankings and
college admissions proxies makes the process manipulable. Students can
hire consultants to coach their way through the process. As more and
more students do this, admissions rates plummet, making the schools
even more attractive to the next round of high school graduates. Even
the more complex proxies, such as essays, could be gamed if they were
evaluated algorithmically as in Pennebaker's work. The answer is not
more algorithms. The answer is to start unwinding the rankings-fuelled
feedback loop which generates such an unreasonable number of
applicants.


	<p><b><font color="red">
	      Nice specifics (college admissions, SAT scores).   The topic sentence is nicely connected to those specifics.  It is nice
	      to see the abstract ending with a suggestion on how to go forward.
</font></b></p>
  </li>

        <li>
<h2>Beware of Biases</h2>

Machine learning models can sometimes be severely biased and
vulnerable in real-life settings. First, models learn and even amplify
implicit bias in the training data. For example, biases towards race
and gender are found in the widely used GloVe word-embedding, which
would be harmful when making related decisions. Furthermore, models
that generally performance well may fail on some intentionally
constructed instances. The problem of adversarial examples makes
neural network models vulnerable to malicious attack. Although recent
researches make visualizing and explaining the machine learning models
possible, the crucial problem of biasness and vulnerability still
remains unsolved.

	<p><b><font color="red">
	      I agree.  The problem of bias remains unsolved.  Point is clearly stated in conclusion, and well introduced by topic sentence.
	      I wish we had a more constructive suggestion forward, but it is useful to get past denial.
</font></b></p>
  </li>

	  <li>
<h2>Algorithms and Discrimination</h2>

Those mathematics models that involves invisibly in every part of our
lives are implemented by exquisite algorithms and comprehensive data,
and claim to build the better world and eliminate the bias, whereas in
fact reinforce the discrimination and injustice in the society.  Using
objective measurement instead of subjective judgment sometimes doesn't
helpful for maintaining a fairness of society. The weapon of an
algorithm is trying to use a bunch of abuse mathematics models and
equations to pick up the valuable target efficiently and to conceal
the unfair judge selecting intention. However, the "well-designed"
algorithm could also make the great contributions. Therefore, the
modelers and algorithm designers need to take care about how to use
the data or scores correctly, and policy makers need to pay attention
to those companies using the models and to make sure they using it
regulatory and without bias.

<p><b><font color="red">
      I agree that much needs to happen.  The challenge is how to get there from here.
      The point (that much needs to happen) is clearly stated in the conclusion, and well introduced in the topic sentence.
</font></b></p>
  </li>

	    <li>
	      <h2>Fairness in Models of Language</h2>

	      With the growing applications of statistical and
	      machine-learned systems, there have also come growing
	      concerns with the fairness, or lack thereof, of these
	      systems. In this survey, we aim to bring these concerns
	      to light specifically as they relate to NLP
	      technologies. We highlight the ways in which NLP
	      technologies can cause damage to the speaker, mention,
	      and listener of the language which is analyzed and
	      produced. We survey existing work that has shown
	      stereotypes consistently surfacing in models of
	      language. Because of the nature of language as a
	      reflection of the society, we argue that these
	      stereotypes will occur by default in any model of
	      language, unless actively fought against. We then
	      present methods proposed by the machine learning
	      community for fighting against unwanted bias, and urge
	      continued work to build upon these for models of
	      language.

	<p><b><font color="red">
I agree.  Benign neglect won't get the job done.  We can't ignore the problem.  But can you offer some specifics for
how to fight back?
</font></b></p>
  </li>
</ol>
  </body>
</html>
